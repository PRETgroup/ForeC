\subsection{Comparison with Esterel, \pretc{}, and Concurrent Revisions}
\label{sec:forec_comparison}

\begin{table}
	\centering
	\tbl{Comparing ForeC with Esterel, \pretc{}, and Concurrent revisions.\label{tab:forec_comparison}}{
		\begin{tabular}{| p{3.1cm} || p{1.8cm} | p{1.8cm} | p{2.4cm} | p{2.4cm} |}
			\hline
			\textbf{Property}											& \textbf{Esterel}							& \textbf{\pretc{}}						& \textbf{ForeC}						& \textbf{Concurrent \newline revisions}\\
			\hline
			Causal Programs												& Not always								& \multicolumn{3}{c|}{Yes, by construction}																				\\ \hline
			Use for Parallelism											& Control									& \multicolumn{2}{c|}{Control and data}											& Data									\\ \hline
			Model of Computation										& \multicolumn{3}{c|}{Synchronous}																							& Asynchronous							\\ \hline
			Reactive Interface											& \multicolumn{3}{c|}{Yes}																									& No									\\ \hline
			Preemption													& \multicolumn{3}{c|}{Yes}																									& No									\\ \hline
			Parallelism is Dynamic										& \multicolumn{3}{c|}{No}																									& Yes									\\ \hline
			Thread Communication Method									& Pure and valued signals					& \multicolumn{3}{c|}{Shared Variables}																					\\ \hline
			Thread Communication Speed									& Instantaneous								& Instantaneous (sequential)			& Delayed to the end of each tick		& Delayed to thread termination			\\ \hline
			Resynchronization of shared variables or valued signals		& Combine functions (\texttt{mod} values)	& Not required							& Combine functions with policies		& Merge functions (\texttt{all} values)	\\ \hline
			Parallelism is Commutative and Associative					& Yes										& No \newline (Sequential)				& Depends on combine functions			& Depends on merge functions			\\
			\hline
		\end{tabular}
	}	
\end{table}

This section compares ForeC with
Esterel~\cite{timed_esterel}, \pretc{}~\cite{pret_pretc}, and Concurrent
revisions~\cite{BurckhardtL11} and Table~\ref{tab:forec_comparison}
summarizes this qualitative comparison. Concurrent revisions is 
a programming model that supports the forking and joining of asynchronous 
threads. When a thread is forked, a snapshot of the shared variables is 
created and any changes performed by the thread are only applied to its 
snapshot. This ensures thread isolation during execution. When two threads
join, their snapshots are merged together using a deterministic \emph{merge 
function}.

ForeC and \pretc{} are intended for applications that have
control and data parallelism. Control parallelism is not a
strength of concurrent revisions because its semantics does
not consider (reactive) inputs and outputs. 
In Concurrent revisions, 
threads are forked asynchronously, allowing a parent thread 
to execute alongside its children. Hence, the parent thread 
can vary the amount of parallelism that is needed at runtime 
(e.g., fork more threads when there are more input data).
The \texttt{rjoin} construct can be used to force the parent 
thread to wait for its children to terminate.
In contrast, threads are forked synchronously in ForeC, Esterel, 
and \pretc{}, meaning that the parent thread blocks until of all its
children have terminated. Hence, the parent thread cannot
vary the amount of parallelism at runtime.

Threads in \pretc{} are executed in a strict sequential order, 
which is unsuitable for multi-core execution. However, the strict order
ensures that only one thread is executing at any time and 
that shared variables are accessed in a thread safe manner. 
Consequently, thread communication is instantaneous in the 
sequential order (instantaneous in the synchronous model of 
computation, that is, occurring in the same global tick), 
but delayed by one tick in the reverse order.
Similar to ForeC, threads in Concurrent revisions
communicate over shared variables. When a child thread is
forked, it creates a snapshot of the shared variables from its
parent thread. When the child thread joins back with its
parent, the snapshots of both threads are merged with a
programmer-specified \emph{merge function}. The merge function always
considers both copies, i.e., equivalent to ForeC's combine
policy \verb$all$. Thus, thread communication is always
delayed until the child thread terminates. In contrast, 
ForeC threads may execute over several ticks and thread
communication is only delayed to the end of each tick.
Esterel threads communicate instantaneously by emitting and
receiving pure or valued signals during each tick. Pure
signals are either present or absent and carry no value.
All potential signal emissions must be performed by the 
concurrent threads before 
the signal status (present or absent) can be read. 
Compilers~\cite{distributed_reactive_systems_survey,timed_cec,YuanYR11} 
typically compile away the parallelism to create a sequential
program that resolves the causality, but this restricts the 
execution onto single cores.
Valued signals are like pure signals except that each emission
has an associated value. Before a valued signal can be read,
all the emitted values are combined using a
programmer-specified commutative and associative combine
function. The combine function only considers the
emitted values, i.e., equivalent to ForeC's combine policy \texttt{mod}. 

Esterel's parallel construct for forking threads is
commutative and associative, thanks to the requirement that
all combine functions must be commutative and associative.
\pretc{}'s parallel construct is not commutative or associative 
because threads communicate in a strict sequential order.
For ForeC and Concurrent revisions, the commutativity and
associativity of their parallel construct depends on their
combine and merge functions, respectively. 

Preemptions in ForeC and \pretc{} are inspired by
Esterel, but behave slightly differently. Preemptions in
Esterel are triggered instantaneously, whereas preemptions
in ForeC are triggered with a delay of one tick. 
Preemptions in \pretc{} are triggered instantaneously, but 
the non-immediate behavior is not supported.
Concurrent revisions do not support preemptions.
Esterel programs may be non-causal~\cite{timed_synchronous_survey} 
because of instantaneous feedback cycles. Thanks to delayed
communication, ForeC and Concurrent revisions programs are
always causal by construction. \pretc{} programs are always
causal by construction because threads communicate in a 
strict sequential order.


\subsection{Discussion}
This section has introduced the ForeC language that enables
the deterministic parallel programming of multi-cores. The
language features of ForeC help bridge the differences
between synchronous-reactive programming and general-purpose
parallel programming. 
ForeC makes deterministic parallelism
accessible to traditional embedded C programmers. ForeC
offers shared variable semantics that removes the burden of
ensuring mutual exclusion from the programmer and guarantees
deadlock freedom. Thread isolation is guaranteed by
stipulating that threads work on local copies of the shared
variables. Resynchronizing the shared variables when the
threads have finished their respective local ticks makes
program behavior agnostic to scheduling decisions.
These features simplify the understanding and debugging of ForeC
programs. Important definitions and proofs for ForeC were
given for reactivity and determinism.
Finally, a critical comparison showed that ForeC merges the
benefits offered by synchronous languages, such as 
Esterel~\cite{timed_esterel}, with those offered by deterministic 
runtime solutions such, as Concurrent revisions~\cite{BurckhardtL11}.

Traditional synchronous programming languages~\cite{timed_synchronous_survey} are
notoriously difficult to distribute or 
parallelize~\cite{distributed_reactive_systems_survey} due to
their signal communication model. The key advantage of the
synchronous model of computation is that it 
removes the need to use thread
synchronization mechanisms such as mutual exclusion.
However, the need to maintain monotonic signal
values~\cite{YuanYR11,multiprocessing_openmp_synchronous} 
makes it very difficult to parallelize these programs. 
Moreover, static analysis is needed to ensure that the 
presence or absence of all signals can be determined 
exactly in each tick of the program (a corollary is that
programs can react to the \emph{absence} of signals). 
In contrast, communication in ForeC is delayed using
shared variables, the values which
are only resolved when threads complete their local 
ticks, hence allowing threads to execute in parallel
and in isolation (but preventing the reaction to 
absence). 

Section~\ref{sec:forec_compiling}
presents a straightforward compilation approach
for ForeC. Benchmarking in 
Section~\ref{sec:forec_benchmarking} reveals that our
compilation approach offers good parallel execution that
is amenable to static timing analysis. To our knowledge,
no other synchronous language achieves parallel
execution and timing predictable as good as ForeC.

ForeC's combine functions are inspired by 
Esterel~\cite{timed_compiling_esterel} but similar 
solutions can be found 
in other parallel programming frameworks, e.g., OpenMP's
\verb$reduction$ operators~\cite{multiprocessor_openmp},
MPI's \verb$MPI_Reduce$ and \verb$MPI_Gather$ functions~\cite{multiprocessor_mpi},
Intel Thread Building Blocks' \verb$tbb::parallel_reduce$
function and \verb$tbb::combinable$ data type~\cite{multiprocessor_intel_tbb},
Intel Cilk Plus' \verb$reducer$ data types~\cite{multiprocessor_intel_cilk_plus}, 
and Unified Parallel C's collective functions~\cite{multiprocessing_upc}.
Solutions developed for these frameworks could be 
reworked into ForeC combine functions.
Appendix~\ref{sec:forec_combine} provides more extensive examples
of combine functions. A description of how the combine policies
and combine functions work together to combine more than two copies
of a shared variable is given.

Determinism in ForeC is guaranteed by the formal semantics, and 
preserved by the compiler. This is unlike the
deterministic runtime solutions developed for 
Pthreads~\cite{multiprocessing_grace,multiprocessing_kendo,multiprocessing_coredet,multiprocessing_dthreads},
OpenMP~\cite{multiprocessing_domp}, and 
MPI~\cite{multiprocessing_detmp}, where determinism is only enforced at
runtime and can be sensitive to changes in the program
code. However, the behaviors of the deterministic runtimes are not 
described with formal semantics. Moreover, program execution in Pthreads, OpenMP, and MPI 
is not portable across the runtime solutions because each deterministic
runtime enforces its own notion of
determinism. 
