\section{Related Work}
\label{sec:literature}
Designing embedded systems that are time-predictable remains 
an open challenge~\cite{AxerEFGGGJMRRSHWY14}. Moreover, the 
growth of embedded multi-cores is pushing more
programmers to be parallel programming experts. 
Table~\ref{table:literature:mutual_exclusion} 
highlights different approaches for enforcing \emph{mutual
exclusion} on shared variables, usually by interleaving 
the parallel accesses to enforce a sequence of accesses to 
the \emph{critical sections}.
As argued by Lee~\cite{multiprocessing_problem_threads}, 
the adoption of parallelism in sequential languages, like C~\cite{programming_languages_c11}, 
discards important properties, such as determinism, predictability,
and understandability. Thus, programmers spend large 
amounts of time taming the non-determinism in their parallel 
programs~\cite{multiprocessing_debugging_concurrency_study}. 
Instruction reordering is regularly employed by compilers 
and processor cores to maximize execution parallelism, but this
can cause wrong values for shared variables to be observed.
C provides the programmer with memory fences to enforce a partial 
ordering on variable accesses between threads: all side-effects of 
a \emph{releasing} thread are committed before the \emph{acquiring} 
thread leaves the fence. To help tame non-determinism, runtime environments 
that enforce deterministic thread scheduling and memory
accesses can be used. Such runtime environments have been 
developed for Linux processes (DPG~\cite{multiprocessing_dos}), 
Pthreads (Grace~\cite{multiprocessing_grace}, 
Kendo~\cite{multiprocessing_kendo}, CoreDet~\cite{multiprocessing_coredet}, 
and Dthreads~\cite{multiprocessing_dthreads}), OpenMP 
(DOMP~\cite{multiprocessing_domp}), and 
MPI (DetMP~\cite{multiprocessing_detmp}).
For DPG, Kendo, CoreDet, and Dthreads, all thread interactions
are mapped deterministically onto a logical timeline (which
progresses independently of physical time). Program execution
is divided into alternating parallel and serial phases, similar to
the Bulk Synchronous Parallel (BSP)~\cite{Valiant90} programming model. In 
the parallel phase, threads execute in parallel until they all reach
one of the following synchronization points: a lock, memory access, 
or statically defined number of executed instructions. 
Then, in the serial phase, threads take turns to resolve their memory 
accesses or lock acquisitions. Threads in CoreDet and Dthreads 
also maintain their own version of the shared memory state, which
is resynchronized in every serial phase. This concept 
is used and formally defined in concurrent revisions~\cite{BurckhardtL11}. 
DOMP and Grace differ in that the resynchronization only 
occurs when threads reach a synchronization construct.
However, understanding the program's behavior at compile time 
remains difficult because the determinism is only enforced at 
runtime. Thus, if the program is modified, e.g., to fix a 
bug, then a vastly different runtime behavior is possible.  
An alternative is to directly extend and 
modify the C language with deterministic parallelism, such as
SharC~\cite{multiprocessing_data_race_detection}, 
\textsc{CAT}~\cite{multiprocessing_cat}, 
SHIM~\cite{multiprocessing_shim_scheduling}, 
$\Sigma$C~\cite{multiprocessing_sigmac}, and
ForkLight~\cite{multiprocessing_forklight}. These solutions 
allow the asynchronous forking and synchronized joining 
of threads, but lack a convenient mechanism for preempting 
groups of threads. However, their timing predictability 
has not been demonstrated, which is required for programming 
safety-critical embedded systems.

\begin{table}
	\def\arraystretch{1.3}

	\tbl{Existing solutions for avoiding race conditions.\label{table:literature:mutual_exclusion}} {
		\begin{tabular}{| p{\textwidth} |}
			\hline
			\textbf{Programming Constructs:}
			These are constructs written in the host language to provide mechanisms for the 
			programmer to achieve mutual exclusion. Examples include: locks, monitors, memory fences,
			transactional memory, message passing, and parallel data structures. Using these 
			constructs correctly can be tedious and error prone for large programs and may lead to other 
			errors~\cite{multiprocessing_problem_threads,multiprocessing_debugging_concurrency,multiprocessing_debugging_concurrency_study}, 
			e.g., deadlocks, starvation, or priority inversion.									\\ \hline
		
			\textbf{Language Semantics:}
			The language semantics can have a memory model that defines how threads interact 
			through memory, what value a read can return, and when the value of a write becomes 
			visible to other threads. Although the memory model can prevent race conditions, it 
			may only be suitable for a few types of applications. Examples include: synchronous 
			languages~\cite{timed_synchronous_survey}, \pretc{}~\cite{pret_pretc}, Synchronous 
			C~\cite{timed_synccharts_c_proposal}, SharC~\cite{multiprocessing_sharc}, 
			Deterministic Parallel Java~\cite{multiprocessing_dpj}, SHIM~\cite{multiprocessing_shim_cell}, 
			$\Sigma$C~\cite{multiprocessing_sigmac}, concurrent revisions~\cite{BurckhardtL11}, and 
			Reactive Shared Variables~\cite{timed_reactivec_shared_variables}. 					\\ \hline
		
			\textbf{Static Analysis:}
			A compiler or static analyzer can identify and alert the programmer regarding the race 
			conditions in their program (e.g., Parallel Lint~\cite{parallel_lint}) and may try 
			to resolve them by serializing the parallel accesses for the programmer 
			(e.g., Sequentially Constructive Concurrency~\cite{timed_seq_concurrency}). However, 
			programmer guidance is needed for race conditions that cannot be resolved.			\\ \hline
		
			\textbf{Runtime Support:}
			Programs are executed on a runtime layer that dynamically enforces deterministic 
			execution and memory accesses. Examples include: dOS~\cite{multiprocessing_dos},
			Grace~\cite{multiprocessing_grace}, Kendo~\cite{multiprocessing_kendo}, 
			CoreDet~\cite{multiprocessing_coredet}, Dthreads~\cite{multiprocessing_dthreads}, 
			DOMP~\cite{multiprocessing_domp}, and DetMP~\cite{multiprocessing_detmp}. However, 
			understanding the program's behavior at compile time remains difficult because the 
			determinism is only enforced at runtime.											\\ \hline
		
			\textbf{Hardware Support:}
			Parallel accesses can be automatically detected and resolved by the hardware, preventing
			race conditions from happening. Examples include: Ultracomputer's combine hardware~\cite{Schwartz80} 
			and certain shared bus arbitration (e.g., round-robin, TDMA, and priority). However, the 
			timing of the parallel accesses affects how they are interleaved.					\\
			\hline
		\end{tabular}
	}
\end{table}

The classic synchronous languages
are Esterel~\cite{timed_esterel}, Lustre~\cite{timed_lustre}, 
Signal~\cite{timed_signal}, and the recent extension based 
on functional programming such as Lucid Synchrone~\cite{ColacoP03}, 
and are well suited to the modeling 
of control-dominated systems~\cite{CaspiM05} and
safety-critical systems~\cite{timed_synchronous_survey}. 
To increase their uptake with embedded programmers, 
C-based synchronous languages have
been developed, such as Reactive Shared Variables~\cite{timed_reactivec_shared_variables}, 
Esterel C Language (ECL)~\cite{timed_ecl}, \pretc{}~\cite{pret_pretc} and 
\synchronousc{} (SC)~\cite{timed_synccharts_c_proposal,timed_seq_concurrency}. 
The inherent sequential execution semantics of SC, 
Reactive Shared Variables, and \pretc{} renders them 
unsuitable for multi-core execution.
Moreover, concurrency in synchronous languages is a logical concept 
to help the programmer handle concurrent inputs, rather
than a specification for parallel execution.
Thus, compilers typically generate only sequential 
code~\cite{timed_cec,timed_compiling_esterel}, 
although some generate concurrent 
tasks~\cite{CaspiSST08,NataleGZS10,PagettiFBCL11,NataleZ12}
for execution on single-cores. Yuan et al.~\cite{YuanYR11,Yuan13} 
offer a static and dynamic scheduling approach for Esterel
on multi-cores. For the static approach, threads are statically
load-balanced across the cores and signal statuses are resolved 
at runtime. For the dynamic approach, threads that need to be
scheduled for execution are inserted into a custom hardware 
queue accessible to all cores. The dynamic approach has been shown 
to provide better average-case performance compared to the static 
approach~\cite{Yuan13}. This is because the static approach 
uses worst-case execution times to load-balance the threads, even 
though the actual execution times may be shorter.

The common approach for parallelizing synchronous programs is to 
parallelize an intermediate representation of the sequentialized 
code~\cite{distributed_reactive_systems_survey,wcrt_esterel_multicores,YuanYR11,distributed_synchronous_dependency_driven,distributed_reactive_systems_automatic,timed_esterel_distribution_emperor,timed_multiclock_multithreaded}.
Multi-threaded OpenMP programs can be generated from the Synchronous 
Guarded Actions intermediate format~\cite{multiprocessing_openmp_synchronous}.
The techniques differ in the heuristics used to partition
and distribute the program to achieve sufficient parallelism.
The Synchronized Distributed Executive 
(SynDEx)~\cite{distributed_synchronous_semantics_preserving} 
approach considers the cost of communication when distributing
code to each processing element. When distributing a synchronous 
program, some desynchronization~\cite{BenvenisteCG00,GiraultNP06,distributed_synchronous_desynchronize_modes} 
is needed among the concurrent threads. That is, the concurrent 
threads execute at their own pace, but sufficient inter-thread 
communication is used to preserve the original synchronous 
semantics. The use of \emph{futures} has been proposed as a method for 
desynchronizing long computations in 
Lustre~\cite{multiprocessing_lustre_futures}. A \emph{future}
is a proxy for a result that is initially unknown but becomes 
known at a later time and can be computed in parallel with 
other computations. 

Once a synchronous program is implemented, it is necessary
to validate the synchrony hypothesis. 
That is, the worst-case execution time~\cite{Wilhelm14,wcet_methods_survey} 
(WCET) of any global tick must not exceed the minimal 
inter-arrival time of the inputs. This is known as 
worst-case reaction time (WCRT) 
analysis~\cite{wcrt_concurrent_reactive,wcrt_algebra_interfaces} and
various techniques have been developed for 
single-cores~\cite{wcrt_algebra_interfaces,JuHRC12,WangRA13,RaymondMPC13,wcrt_taxys,AndalamRG11,wcrt_concurrent_reactive,KuoSR11} 
and multi-cores~\cite{wcrt_esterel_multicores,YipRAG13}.


%=========================================================================

\subsection{Discussion}
This section has presented a snapshot of the current efforts
in the programming of time-predictable CPSs.
Many of the attempts at providing deterministic
parallelism have used concepts found in
synchronous languages. C-based synchronous languages
have much to offer to embedded programmers in terms of
deterministic concurrency and formally verifiable
implementations, but lack support for parallel execution.
This paper tackles the lack of a C-based synchronous
parallel programming language that offers \emph{both} 
time-predictability and good parallel execution performance.
