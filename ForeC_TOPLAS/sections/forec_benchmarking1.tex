\section{ForeC Benchmarking}
\label{sec:forec_benchmarking}
This section quantitatively assesses ForeC's parallel
execution performance on a mixture of data and control
dominated benchmark programs. ForeC's execution performance
is compared with that of Esterel, a widely used synchronous
language for concurrent safety-critical systems that has 
inspired some features of ForeC, and that of
OpenMP, a popular desktop solution for parallel programming.
The static timing analysis of ForeC using the reachability
technique is described in a previous paper~\cite{YipRAG13}. 
The benchmark results~\cite{YipRAG13} showed that the worst-case
reaction time~\cite{wcrt_concurrent_reactive} (WCRT) of 
ForeC programs could be estimated to a high degree of precision,
which is very useful for implementing real-time embedded systems in
general, and time-predictable systems in particular. 
We highlight some of the key findings in this section.


\subsection{Benchmark Programs}
This section describes the benchmark programs used in
the evaluations:
\begin{itemize}
	\item \texttt{FlyByWire} is based on the real-time UAV benchmark 
		  called PapaBench~\cite{benchmark_papabench}. \verb$FlyByWire$
		  is a control dominated program with several tasks 
		  managing the UAV's motors, navigation, timer, and 
		  operation mode.
		  
	\item \texttt{FmRadio} \cite{streaming_openmp_extension} is 
		  based on the GNU Radio Package~\cite{benchmark_gnu_radio}, which 
		  transforms a fixed stream of radio signals into 
		  audio. The history of the radio signals 
		  is used to determine how the remaining stream of 
		  signals should be transformed. \verb$FmRadio$ is data orientated.
		  
	\item \texttt{Life} simulates Conway's Game of Life~\cite{Gardner70} for a fixed
		  number of iterations and a given grid of cells. In each iteration
		  of the simulation, the outcome of each cell can be computed 
		  independently. \verb$Life$ has a good 
		  mixture of data and control dominated computations.
		  
	\item \texttt{Lzss} uses the Lempel-Ziv-Storer-Szymanski (LZSS~\cite{benchmark_lzss}) 
		  algorithm to compress a fixed amount of text. Multiple
		  sliding windows are used to search different parts of the 
		  text for repeated segments that can compressed. \verb$Lzss$ has
		  a good mixture of data and control dominated computations.
		  
	\item \texttt{Mandelbrot} computes the Mandelbrot
		  set for a square region of the complex number plane. The
		  Mandelbrot set for each point in the region can be computed 
		  independently, making \verb$Mandelbrot$ a data-parallel program.
		  
	\item \texttt{MatrixMultiply} computes the matrix
		  multiplication of two equally sized square matrices. Each
		  element in the resulting matrix can be computed independently,
		  making \verb$MatrixMultiply$ a data-parallel program.
\end{itemize}


\subsection{Performance Evaluation}
\label{sec:forec_benchmarking:performance}
The performance of ForeC is evaluated against that of Esterel,
a traditional synchronous language, and OpenMP,
a general-purpose parallel programming extension to C. 
We create C (non-multi-threaded), ForeC, 
Esterel, and OpenMP versions of each benchmark program 
and handcrafted each for best performance. 
We use \emph{Speedup} as the performance metric to compare ForeC, 
Esterel, and OpenMP:
\begin{equation*}
	Speedup(P) = \frac{\text{Execution time of the C sequential version}}{\text{Execution time of }P}
\end{equation*}
where $P$ is either the ForeC, Esterel, or OpenMP version of 
the benchmark program being tested.
Hence, the speedups of ForeC, Esterel, and OpenMP are always
with respect to the execution time of the C version.
The higher the speedup the better.


\subsubsection{Comparison with Esterel}
The approaches by Yuan et al.~\cite{YuanYR11,Yuan13} for
parallelizing the execution of Esterel programs has been shown
to perform well on an Intel multi-core and on a
(simulated) Xilinx MicroBlaze multi-core. However, only
compiler support is available for Yuan et al.'s
dynamic scheduling approach on MicroBlaze multi-core. Thus,
we evaluate ForeC against Esterel on a MicroBlaze
multi-core and use the dynamic scheduling approach to
parallelize the Esterel programs. The static scheduling
approach presented in Section~\ref{sec:forec_compiling} is used
to parallelize the ForeC programs. 
Yuan et al.'s dynamic scheduling approach uses a special hardware 
FIFO queue to allocate the threads to the cores. Each core retrieves 
a thread from the queue and executes it until it terminates or 
reaches a context-switching point for resolving signal statuses. 
A core makes a context-switch by adding the executing thread back 
to the queue and retrieving a different thread from the queue. 
Threads are added to the queue when they are forked by other threads. 
Threads are removed from the queue when they terminate. All the cores 
can access the FIFO queue in parallel and each access takes two clock 
cycles to complete. For benchmarking, the MicroBlaze 
multi-core simulator described in 
Section~\ref{sec:introduction:pret:multicore} is extended 
with a hardware queue to support the dynamic scheduling. 
The configuration of the simulator is shown in 
Figure~\ref{fig:results_embedded_specs}. 

\begin{figure}
	\centering
	\def\arraystretch{1.3}
	\begin{tabular}{|p{\textwidth}|}
		\hline
		Xilinx MicroBlaze, 4 physical cores, three-stage pipeline, no speculative	
		features (no branch prediction,	caches, or out-of-order execution), 
		16 KB private data and instruction scratchpads on each core (1 cycle access time), 
		64 KB global memory (5 cycle access time), TDMA shared bus (5 cycle time slots per			
		core), Benchmarks compiled with GCC-4.1.2 -O0.
		\\ \hline
	\end{tabular}
	
	\caption{MicroBlaze multi-core configuration.}
	\label{fig:results_embedded_specs}
\end{figure}

\begin{table}
	\centering
	\def\arraystretch{1.3}
	\tbl{ForeC versus Esterel benchmarks.\label{tab:results_embedded_prog}}{
		\begin{tabular}{l c c l c c}
			\hline
									& \multicolumn{2}{c}{\bf Lines of Code}		& & \multicolumn{2}{c}{\bf Number of Threads}	\\ \cline{2-3}\cline{5-6}
			{\bf Benchmark}			& {\bf ForeC}	& {\bf Esterel}				& & {\bf ForeC}	& {\bf Esterel}					\\
			\hline
			\texttt{Life}			& 212			& 139+111					& & 4 (4)		& 7 (4)							\\ 
			\texttt{Lzss}			& 485			& 42+421					& & 4 (4)		& 4 (4)							\\ 
			\texttt{Mandelbrot}		& 381			& 220+337					& & 8 (8)		& 18 (9)						\\ 
			\texttt{MatrixMultiply}	& 162			& 51+53						& & 16 (8)		& 16 (8)						\\ 
			\hline
		\end{tabular}
	}
\end{table}

Table~\ref{tab:results_embedded_prog} shows the
implementation details of the ForeC and Esterel versions of
the benchmark programs. Esterel is suited for specifying
control concurrency, but is not so for specifying
data dominated computations. Esterel allows data 
computations to be delegated to external host functions,
defined in a host language such as C. Hence, for the ``Lines
of Code'' column in Table~\ref{tab:results_embedded_prog},
the first number is the lines of Esterel code and the second
number is the lines of host C-code (excluding header files).
The ``Number of Threads'' column specifies the total number of
threads forked by the programs and, in brackets, the total
number of threads that can execute together in parallel. The
benchmark programs are compiled for bare-metal execution
and do not need operating system support. Yuan et al.'s
compilation approach~\cite{Yuan13} uses an intermediate
format called GRaph Code (GRC)~\cite{timed_compiling_esterel}, which
transforms the program into an acyclic execution graph. The
GRC helps schedule the resolution of signals, executing 
the GRC from the top to the bottom corresponds to one tick
of the program. To decide which GRC states need to be executed during
each tick, a set of internal variables are updated as
the GRC is executed. Compared to GRC, for most programs, the 
ForeC compiler (see Section~\ref{sec:forec_compiling})
can generate more efficient code that require less context-switching.
The same input vector is given to the ForeC and
Esterel versions of a benchmark program to ensure
that the same computations are performed. When a program
terminates, the simulator returns the execution
time in clock cycles.

Figure~\ref{fig:results_embedded} shows the speedups
achieved by ForeC and Esterel when the benchmark programs
execute on four cores. Apart from \verb$MatrixMultiply$,
ForeC shows superior performance compared to Esterel, even
though Esterel uses dynamic scheduling with hardware
acceleration. The need to resolve instantaneous signal
communication in Esterel can lead to significant runtime
overheads. All possible signal emitters must execute before
any signal consumers can execute and this invariant is 
achieved using a signal locking protocol~\cite{Yuan13} that 
is costly. In comparison, shared
variables in ForeC only need to be resolved at the end of
each tick. The significance of the overhead is
evident in the \verb$Mandelbrot$ results, where the Esterel version
has 24 unique signals and only achieves a speedup of
1.2$\times$ on four cores. In fact, when \verb$Mandelbrot$ is
executed on one core, Esterel's execution time is already 58\%
longer than the C version. ForeC's execution time is only
0.2\% longer than the C version. For \verb$MatrixMultiply$, the
fork-join pattern was used by ForeC and Esterel. Because of
minimal data dependencies in \verb$MatrixMultiply$, combine functions
are not needed in the ForeC version and signals
are not needed in the Esterel version. Thus, the scheduling
overheads for the ForeC and Esterel versions are minimal,
resulting in very similar speedup values. 

\begin{figure}
	\centering

	\begin{tikzpicture}
		\begin{axis}[
			ybar = 0,
			footnotesize,
			width = 10.1cm, height = 6cm,
		%
			ylabel = Average Speedup Normalized to Baseline (Sequential),
			ylabel style = {align = center, text width = 6cm},
			ymax = 4, ymin = 0,
			ytick = {\pgfkeysvalueof{/pgfplots/ymin}, ..., \pgfkeysvalueof{/pgfplots/ymax}},
			ymajorgrids = true,
		%
			xtick = data,
			symbolic x coords = {Life, Lzss, Mandelbrot, MatrixMultiply},
			xtick pos = left,
			enlarge x limits = {abs = 1cm},
		%
			legend columns = -1,
			legend style = {at = {(0.5, 1.05)}, anchor = south},
			legend image code/.code = {\draw[#1] (0cm, -0.12cm) rectangle (0.6cm, 0.15cm);}  
		]
			\addplot[blue, pattern = horizontal lines, pattern color = blue] table[x = program, y = speedup, row sep = \\] {
				program			speedup	\\
				Life			1		\\ 
				Lzss			1		\\
				Mandelbrot		1 		\\
				MatrixMultiply	1 		\\
			};

			\addplot[red, pattern = north east lines, pattern color = red] table[x = program, y = speedup, row sep = \\] {
				program			speedup	\\
				Life			2.28	\\
				Lzss			2.42	\\
				Mandelbrot		1.20	\\
				MatrixMultiply	3.87	\\
			};

			\addplot[black!30!green, pattern = dots, pattern color = black!30!green] table[x = program, y = speedup, row sep = \\] {
				program			speedup	\\
				Life			3.25	\\ 
				Lzss			3.30	\\
				Mandelbrot		3.68 	\\
				MatrixMultiply	3.76 	\\
			};

			\legend{Baseline (Sequential C), Esterel, ForeC}
		\end{axis}
	\end{tikzpicture}

	\caption{Average speedup results for ForeC and Esterel on four cores normalized to sequential runtime. 
			 Platform details in Figure~\ref{fig:results_embedded_specs}.}
	\label{fig:results_embedded}	
\end{figure}

\begin{figure}
	\centering
	\def\arraystretch{1.3}
	\begin{tabular}{|p{\textwidth}|}
		\hline
		Intel Core-i5 3570 at 3.4 Ghz, 4 physical cores,
		Hyper-Threading disabled, Turbo Boost disabled,
		SpeedStep disabled, 3 MB L3 data-cache,
		Linux 3.6, 8 GB of RAM, Benchmarks compiled with GCC-4.8 -O2.
		\\ \hline
	\end{tabular}
	
	\caption{Intel multi-core configuration.}
	\label{fig:results_desktop_specs}
\end{figure}

\subsubsection{Comparison with OpenMP}
Table~\ref{tab:results_desktop_prog} shows the
implementation details of the ForeC and OpenMP versions of
the benchmark programs. Intel VTune Amplifier XE 2013~\cite{vtune} 
software was used during the development and testing of the
parallelized benchmarks. The software is useful in
providing insight into the regions of code which are the
most time consuming and therefore top candidates for
parallelization. It is also used to calculate the total
runtimes and to show the number of cores being used 
during the execution of each benchmark.
Figure~\ref{fig:results_desktop_specs} shows the
specifications of the desktop computer on which all testing
is carried out.

\begin{table}
	\centering
	\def\arraystretch{1.3}
	\tbl{ForeC versus OpenMP benchmarks.\label{tab:results_desktop_prog}}{
		\begin{tabular}{l c c l c}
			\hline
									& \multicolumn{2}{c}{\bf Lines of Code}	&	& {\bf Number of Threads}	\\ \cline{2-3}\cline{5-5}
			{\bf Benchmark}			& {\bf ForeC}	& {\bf OpenMP}			&	& {\bf ForeC}				\\
			\hline
			\texttt{FlyByWire}		& 241			& 227					&	& 8 (7)						\\ 
			\texttt{FmRadio}		& 481			& 382					&	& 12 (6)					\\ 
			\texttt{Life}			& 325 			& 268					&	& 10 (8) 					\\ 
			\texttt{Lzss}			& 593 			& 552					&	& 4 (4) 					\\ 
			\texttt{Mandelbrot}		& 111 			& 89					&	& 4 (4)						\\ 
			\texttt{MatrixMultiply}	& 156			& 121					&	& 7 (4)						\\ 
			\hline
		\end{tabular}
	}	
\end{table}

\begin{figure}
	\centering

	\begin{tikzpicture}
		\begin{axis}[
			ybar = 0,
			footnotesize,
			width = \columnwidth, height = 6cm,
		%
			ylabel = Average Speedup Normalized to Baseline (Sequential),
			ylabel style = {align = center, text width = 6cm},
			ymax = 4, ymin = 0,
			ytick = {\pgfkeysvalueof{/pgfplots/ymin}, ..., \pgfkeysvalueof{/pgfplots/ymax}},
			ymajorgrids = true,
		%
			xtick = data,
			symbolic x coords = {FlyByWire, FmRadio, Life, Lzss, Mandelbrot, MatrixMultiply},
			xtick pos = left,
			enlarge x limits = {abs = 1cm},
		%
			legend columns = -1,
			legend style = {at = {(0.5, 1.05)}, anchor = south},
			legend image code/.code = {\draw[#1] (0cm, -0.12cm) rectangle (0.6cm, 0.15cm);}  
		]
			\addplot[blue, pattern = horizontal lines, pattern color = blue] table[x = program, y = speedup, row sep = \\] {
				program			speedup	\\
				FlyByWire		1		\\
				FmRadio			1		\\
				Life			1 		\\
				Lzss			1		\\
				Mandelbrot		1 		\\
				MatrixMultiply	1 		\\
			};

			\addplot[red, pattern = north east lines, pattern color = red] table[x = program, y = speedup, row sep = \\] {
				program			speedup	\\
				FlyByWire		3.80	\\
				FmRadio			2.02	\\
				Life			2.82 	\\
				Lzss			3.46 	\\
				Mandelbrot		2.04 	\\
				MatrixMultiply	3.35	\\
			};

			\addplot[black!30!green, pattern = dots, pattern color = black!30!green] table[x = program, y = speedup, row sep = \\] {
				program			speedup	\\
				FlyByWire		2.42	\\
				FmRadio			2.63	\\
				Life			2.98 	\\
				Lzss			3.90	\\	
				Mandelbrot		3.88 	\\
				MatrixMultiply	3.90 	\\
			};

			\legend{Baseline (Sequential C), OpenMP, ForeC}
		\end{axis}
	\end{tikzpicture}

	\caption{Average speedup results for ForeC and OpenMP on four cores normalized to sequential runtime. 
			 Platform details in Figure~\ref{fig:results_desktop_specs}.}
	\label{fig:results_desktop}	
\end{figure}

Figure~\ref{fig:results_desktop} shows the speedups achieved by ForeC
and OpenMP when the benchmark programs are executed over four
cores. The speedups are averaged over 200 executions of each program to
take into account the potential effects of long term use, e.g., filling
and flushing of the cache, and background kernel processes on the computer. ForeC
and OpenMP are able to achieve a speedup factor of between two and
four over four cores. However, from the results it is clear that in most
cases ForeC produces a greater speedup factor, barring two
exceptions. Firstly, OpenMP is more suited and delivered a much higher
speedup factor in \verb$FlyByWire$. Secondly, for \verb$Mandelbrot$,
the OpenMP version is unable to utilize all four available cores.

We would like to mention that we used dynamic and static thread
scheduling pragmas in OpenMP. Static scheduling is used in benchmarks
(e.g., \verb$FlyByWire$) when we could determine at compile time the so 
called \emph{chunk size} (the amount of work and number of loop iterations 
that each thread needs to perform). Dynamic scheduling is used in benchmarks 
(e.g., \verb$MatrixMultiply$ and \verb$Mandelbrot$) when the chunk size of 
each thread could not be made equal or could not be determined at compile 
time. For dynamic scheduling, the chunk size of each thread is determined
by the OpenMP runtime. Using dynamic scheduling does introduce slight overheads,
especially thread locking, but these overheads should be amortized over
the overall run of the benchmarks. This OpenMP scheduling approach is in
stark contrast to the ForeC approach, where all work scheduling is
static and determined automatically by the ForeC compiler, whereas in
OpenMP all work scheduling is the programmer's burden.


\subsection{Time Predictability}
We developed a C++ static timing analysis
tool~\cite{YipRAG13}, called ForeCast, that statically
analyzes the WCRT of ForeC programs on embedded multi-core
processors. We highlight the key findings of our previous
paper~\cite{YipRAG13} on the static WCRT analysis of ForeC
programs executed on embedded multi-cores.
Benchmarking is performed on the MicroBlaze multi-core
simulator with the configuration shown in
Figure~\ref{fig:results_embedded_static_specs} and
ForeCast itself is executed on a 2.20 GHz Intel Core 2 Duo
computer with 3 GB RAM and Linux 2.6.38. We
highlight the results of the benchmark program
called \verb$802.11a$~\cite{streaming_openmp_extension}.
\verb$802.11a$ is production code from Nokia that tests
various signal processing algorithms needed to decode
802.11a data transmissions. \verb$802.11a$ has both complex
data and control dominated computations. \verb$802.11a$ has 
2147 lines of ForeC code and forks up to 26 threads, of which 
10 can execute in parallel. \verb$802.11a$ is 
distributed on up to 10 cores and ForeCast is used to 
compute the WCRT of each possible distribution. 
The WCRT computed by ForeCast is taken as the \emph{computed} WCRT.
To evaluate the tightness of the computed WCRTs, \verb$802.11a$ 
is executed on the MicroBlaze simulator for one million 
global ticks or until the program terminates. Test
vectors are generated to elicit the worst-case program state by
studying the program's control-flow. The simulator returns
the execution time of each global tick and the longest is
taken as the \emph{observed} WCRT. 

\begin{figure}
	\centering
	\def\arraystretch{1.3}
	\begin{tabular}{|p{\textwidth}|}
		\hline
		Xilinx MicroBlaze, three-stage pipeline, no speculative	
		features (no branch prediction,	caches, or out-of-order execution), 
		8 KB private data and instruction scratchpads on each core 
		(1 cycle access time), 32 KB global memory 
		(5 cycle access time), TDMA shared bus (5 cycle time slots per			
		core and, thus, a $5\times$(number of cores) 
		cycles long bus schedule), Benchmarks compiled with MB-GCC-4.1.2 -O0 and 
		decompiled with MB-OBJDUMP-4.1.2.
		\\ \hline
	\end{tabular}
	
	\caption{MicroBlaze multi-core configuration.}
	\label{fig:results_embedded_static_specs}
\end{figure}

\begin{figure}
	\centering

	\begin{minipage}[t]{6.5cm}
		\begin{tikzpicture}
			\begin{axis}[
				footnotesize,
				width = 6.5cm, height = 6cm,
				grid = major,
			%
				tick scale binop=\times,
				ylabel = WCRT (clock cycles),
				ymax = 200000, ymin = 0,
				ytick = {0, 25000, 50000, 75000, 100000, 125000, 150000, 175000, 200000},
				scaled y ticks = base 10:-3,
			%
				xlabel = Cores,
				xtick = data,
				xmax = 10, xmin = 1,
				xtick = {\pgfkeysvalueof{/pgfplots/xmin}, ..., \pgfkeysvalueof{/pgfplots/xmax}},
			%
				legend columns = -1,
				legend style = {at = {(0.5, 1.15)}, anchor = south},
			]
				\addplot[red, mark = triangle] table[x = core, y = time, row sep = \\] {
					core	time	\\
					1		152297	\\
					2		94518	\\
					3		70453	\\
					4		60858	\\
					5		53573	\\
					6		56308	\\
					7		64783	\\
					8		72998	\\
					9		75238	\\
					10		83248	\\
				};

				\addplot[blue, mark = o] table[x = core, y = time, row sep = \\] {
					core	time	\\
					1		154306	\\
					2		95545	\\
					3		71390	\\
					4		61765	\\
					5		53930	\\
					6		57605	\\
					7		66470	\\
					8		75005	\\
					9		77630	\\
					10		84505	\\
				};
			
				\legend{Observed, Computed}
			\end{axis}
		\end{tikzpicture}
		\caption{WCRT results for \texttt{802.11a} in clock cycles.}
		\label{fig:forec_benchmarking:benchmark_wcrt}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{6.5cm}
		\begin{tikzpicture}
			\begin{axis}[
				footnotesize,
				width = 6.5cm, height = 6cm,
				grid = major,
			%
				ylabel = Over-estimation (\%),
				ymax = 3.5, ymin = 0,
				ytick = {0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5},
			%
				xlabel = Cores,
				xtick = data,
				xmax = 10, xmin = 1,
				xtick = {\pgfkeysvalueof{/pgfplots/xmin}, ..., \pgfkeysvalueof{/pgfplots/xmax}},
			]
				\addplot[red, mark = triangle] table[x = core, y = time, row sep = \\] {
					core	time	\\
					1		1.3		\\
					2		1.1		\\
					3		1.3		\\
					4		1.5		\\
					5		0.6		\\
					6		2.4		\\
					7		2.6		\\
					8		2.7		\\
					9		3.2		\\
					10		1.5		\\
				};
			\end{axis}
		\end{tikzpicture}
	
		\caption{WCRT over-estimations for \texttt{802.11a}.}
		\label{fig:forec_benchmarking:benchmark_wcrt_over}
	\end{minipage}
\end{figure}

The observed and computed WCRTs of \verb$802.11a$ (in clock
cycles) are plotted as a line graph in
Figure~\ref{fig:forec_benchmarking:benchmark_wcrt}. This
graph shows that the static timing analysis is very
precise, even when the number of cores increases. 
The \emph{over-estimation} of the computed WCRT 
can be calculated as follows:
\begin{equation*}
	\text{WCRT Over-estimation} = \frac{\text{Computed WCRT} - \text{Observed WCRT}}{\text{Observed WCRT}} \times 100\%
\end{equation*}
Figure~\ref{fig:forec_benchmarking:benchmark_wcrt_over} 
exemplifies the WCRT over-estimations for 
\verb$802.11a$ as a line graph. We can see that
ForeCast computes WCRTs that are at most 3.2\% longer than the
observed WCRTs. This shows that extremely time predictable 
systems can be designed with ForeC.

The computed WCRTs of \verb$802.11a$ 
in Figure~\ref{fig:forec_benchmarking:benchmark_wcrt} 
reflect the benefit of multi-core execution. The 
computed WCRT decreases when the number of cores 
is increased from one to five cores. The computed
WCRT at five cores corresponded to the
execution time of one thread which is already allocated to
its own core. Thus, the WCRT could not be improved by
distributing the remaining threads. The WCRT increases after
five cores because of the increasing scheduling overheads and
cost of accessing global memory. These costs reduce the
benefit of multi-core execution. 

\begin{figure}
	\subfloat[\texttt{Life}.] {
		\begin{tikzpicture}
			\begin{axis}[
				footnotesize,
				width = 6.5cm, height = 6cm,
				grid = major,
			%
				tick scale binop=\times,
				ylabel = WCET (clock cycles),
				ymax = 6000000, ymin = 0,
				ytick = {0, 1000000, 2000000, 3000000, 4000000, 5000000, 6000000},
				scaled y ticks = base 10:-6,
			%
				xlabel = Cores,
				xtick = data,
				xmax = 4, xmin = 1,
				xtick = {\pgfkeysvalueof{/pgfplots/xmin}, ..., \pgfkeysvalueof{/pgfplots/xmax}},
			%
				legend columns = -1,
				legend style = {at = {(0.5, 1.15)}, anchor = south},
			]
				\addplot[red, mark = triangle] table[x = core, y = time, row sep = \\] {
					core	time	\\
					1		5332852	\\
					2		2923933	\\
					3		2854608	\\
					4		1608143	\\
				};

				\addplot[blue, mark = o] table[x = core, y = time, row sep = \\] {
					core	time	\\
					1		3707623	\\
					2		1989424	\\
					3		2160169	\\
					4		1125504	\\
				};
				
				\legend{Esterel observed, ForeC observed}
			\end{axis}
		\end{tikzpicture}
	}
	\hfill
	\subfloat[\texttt{Lzss}.] {
		\begin{tikzpicture}
			\begin{axis}[
				footnotesize,
				width = 6.5cm, height = 6cm,
				grid = major,
			%
				tick scale binop=\times,
				ylabel = WCET (clock cycles),
				ymax = 60000000, ymin = 0,
				ytick = {0, 10000000, 20000000, 30000000, 40000000, 50000000, 60000000},
				scaled y ticks = base 10:-7,
			%
				xlabel = Cores,
				xtick = data,
				xmax = 4, xmin = 1,
				xtick = {\pgfkeysvalueof{/pgfplots/xmin}, ..., \pgfkeysvalueof{/pgfplots/xmax}},
			]
				\addplot[red, mark = triangle] table[x = core, y = time, row sep = \\] {
					core	time		\\
					1		49612714	\\
					2		30498189	\\
					3		29296659	\\
					4		18188259	\\
				};

				\addplot[blue, mark = o] table[x = core, y = time, row sep = \\] {
					core	time		\\
					1		44109533	\\
					2		23282661	\\
					3		23769071	\\
					4		13338631	\\
				};
			\end{axis}
		\end{tikzpicture}
	}

	\subfloat[\texttt{Mandelbrot}.] {
		\begin{tikzpicture}
			\begin{axis}[
				footnotesize,
				width = 6.5cm, height = 6cm,
				grid = major,
			%
				tick scale binop=\times,
				ylabel = WCET (clock cycles),
				ymax = 40000000, ymin = 0,
				ytick = {0, 5000000, 10000000, 15000000, 20000000, 25000000, 30000000, 35000000, 40000000},
				scaled y ticks = base 10:-7,
			%
				xlabel = Cores,
				xtick = data,
				xmax = 4, xmin = 1,
				xtick = {\pgfkeysvalueof{/pgfplots/xmin}, ..., \pgfkeysvalueof{/pgfplots/xmax}},
			]
				\addplot[red, mark = triangle] table[x = core, y = time, row sep = \\] {
					core	time		\\
					1		37827004	\\
					2		26236151	\\
					3		21169386	\\
					4		19912651	\\
				};

				\addplot[blue, mark = o] table[x = core, y = time, row sep = \\] {
					core	time		\\
					1		23943425	\\
					2		12375556	\\
					3		9409536		\\
					4		6501186		\\
				};
			\end{axis}
		\end{tikzpicture}
	}
	\hfill
	\subfloat[\texttt{MatrixMultiply}.] {
		\begin{tikzpicture}
			\begin{axis}[
				footnotesize,
				width = 6.5cm, height = 6cm,
				grid = major,
			%
				tick scale binop=\times,
				ylabel = WCET (clock cycles),
				ymax = 60000000, ymin = 0,
				ytick = {0, 10000000, 20000000, 30000000, 40000000, 50000000, 60000000},
				scaled y ticks = base 10:-7,
			%
				xlabel = Cores,
				xtick = data,
				xmax = 4, xmin = 1,
				xtick = {\pgfkeysvalueof{/pgfplots/xmin}, ..., \pgfkeysvalueof{/pgfplots/xmax}},
			]
				\addplot[red, mark = triangle] table[x = core, y = time, row sep = \\] {
					core	time		\\
					1		50404272	\\
					2		26022038	\\
					3		26248728	\\
					4		13027758	\\
				};

				\addplot[blue, mark = o] table[x = core, y = time, row sep = \\] {
					core	time		\\
					1		50934944	\\
					2		26280962	\\
					3		26647997	\\
					4		13417662	\\
				};
			\end{axis}
		\end{tikzpicture}
	}

	\caption{Observed WCETs for ForeC and Esterel. Platform details in Figure~~\ref{fig:results_embedded_specs}.}
	\label{fig:forec_benchmarking:benchmark_wcrt_forec_esterel}
\end{figure}

We perform additional experiments to compare the 
observed worst-case execution times (WCETs) of ForeC and 
Esterel versions of \texttt{Life}, \texttt{Lzss}, \texttt{Mandelbrot},
and \texttt{MatrixMultiply} on embedded
multi-cores. For our experiments, the WCET of a program
is the total time that it takes for the entire program to 
execute from start to finish.\footnote{The entire execution 
of a benchmark program occurs over multiple ticks.} We limit the 
\texttt{Life} program to simulate 10,000 iterations of the game.
The same input vector is given to the ForeC and Esterel versions 
of a benchmark program to ensure that the same computations are performed. 
The Esterel programs are compiled using Yuan
et al.'s approach~\cite{Yuan13}. For each benchmark program in 
Figure~\ref{fig:forec_benchmarking:benchmark_wcrt_forec_esterel}, 
the WCETs for ForeC and 
Esterel are plotted. Apart from \texttt{MatrixMultiply}, the
observed WCETs for ForeC are much shorter than those for Esterel. 
Unfortunately, the static timing
analysis of such multi-core Esterel programs has not been developed, 
preventing an objective comparison of time-predictability. To 
compute WCRTs for Esterel that are as tight as ForeCast, 
the dynamic resolution of signal statuses will need to be analyzed 
extremely carefully to rule out the infeasible runtime decisions.


\subsection{Discussion}
This section assessed the performance 
of ForeC on an embedded multi-core and desktop multi-core. 
The static timing analysis of ForeC programs was presented in
an earlier paper~\cite{YipRAG13}. In this section, 
on an embedded multi-core, most of the statically 
scheduled ForeC programs performed better than Yuan et al.'s~\cite{Yuan13} 
dynamically scheduled Esterel programs. This is because it is easier
to extract parallelism from ForeC programs, largely thanks to its
shared variable semantics (see Section~\ref{sec:forec:shared_variables}). 
Serializing Esterel
programs into GRC can obfuscate the parallelism and the need
to update internal state variables can add unnecessary
overhead. Runtime resolution is also needed to resolve
Esterel's instantaneous signal communication. Ju et
al.~\cite{wcrt_esterel_multicores} provide a multi-core 
static scheduling approach for Esterel. However, we cannot 
compare with that work because speedup results 
for multi-core execution were not reported. 

On a desktop
multi-core, ForeC's static scheduling approach proved to be
competitive against OpenMP, a dynamic runtime solution.
These are encouraging results for the use of ForeC to
develop high performing parallel programs. Moreover, determinism is
enforced by ForeC's formal semantics, not by a particular
runtime environment. There is much scope to improve the
ForeC compiler to generate more efficient code. For example, thread
allocations could be refined automatically by feeding the 
WCRT results of the ForeCast analyzer into the ForeC compiler until 
the WCRT cannot be reduced. 
